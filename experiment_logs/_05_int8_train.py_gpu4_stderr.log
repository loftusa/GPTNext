/disk/u/lofty/GPTNext/experiments/_05_int8_train.py:213: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: aloftus (gptnext). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /disk/u/lofty/GPTNext/wandb/run-20250413_164616-ymklsn4r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run int8_quantization_0413_16:46:15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gptnext/gptnext
wandb: üöÄ View run at https://wandb.ai/gptnext/gptnext/runs/ymklsn4r
/disk/u/lofty/GPTNext/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/disk/u/lofty/GPTNext/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Error invalid configuration argument at line 525 in file /src/csrc/ops.cu
