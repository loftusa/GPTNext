tokens per iteration will be: 262,144
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
number of parameters: 29.94M
AdamW: num decayed parameter tensors: 2, with 19,415,040 parameters
AdamW: num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
Muon: num parameter tensors: 24, with 10,616,832 parameters
using Muon optimizer for 24 tensors
compiling the model... (takes a ~minute)
train throughput: 0.74 M tokens/s in 3.56s
val throughput: 1.72 M tokens/s in 1.53s
step 0: train loss 10.8844, val loss 10.8839
[1;34mwandb[0m: ðŸš€ View run [33mmuon_0413_16:39:55[0m at: [34mhttps://wandb.ai/gptnext/gptnext/runs/n224u2pg[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250413_163958-n224u2pg/logs[0m
