tokens per iteration will be: 262,144
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
number of parameters: 123.59M
AdamW: num decayed parameter tensors: 2, with 39,419,904 parameters
AdamW: num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Muon: num parameter tensors: 48, with 84,934,656 parameters
using Muon optimizer for 48 tensors
compiling the model... (takes a ~minute)
