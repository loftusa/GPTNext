/disk/u/lofty/GPTNext/experiments/_03_rope_train.py:213: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: aloftus (gptnext). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /disk/u/lofty/GPTNext/wandb/run-20250413_164618-nhal89c1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rope_0413_16:46:13
wandb: â­ï¸ View project at https://wandb.ai/gptnext/gptnext
wandb: ğŸš€ View run at https://wandb.ai/gptnext/gptnext/runs/nhal89c1
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          cpu/ram_gb â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–…â–â–â–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–â–â–â–ˆâ–‚â–â–â–…â–â–…
wandb:                       gpu/active_gb â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                      gpu/active_pct â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 gpu/bytes_per_token â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                          gpu/gpu_gb â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                gpu/max_allocated_gb â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                     gpu/reserved_gb â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    gpu/reserved_pct â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        gpu/total_gb â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                iter â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:                                  lr â–†â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                 mfu â–ƒâ–†â–‡â–†â–ˆâ–„â–„â–‚â–„â–†â–„â–…â–†â–„â–„â–…â–†â–ƒâ–…â–‚â–ƒâ–‡â–†â–ƒâ–…â–…â–â–ƒâ–ˆâ–ƒâ–†â–…â–†â–†â–…â–†â–‡â–‚â–ƒâ–ˆ
wandb:   throughput/time_seconds_inference â–
wandb: throughput/tokens_per_sec_inference â–
wandb:     throughput/tokens_per_sec_train â–â–‚â–‚â–„â–„â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   throughput/total_tokens_inference â–
wandb:          train/inference_throughput â–…â–‚â–…â–‡â–‚â–†â–ƒâ–ˆâ–‚â–‚â–ƒâ–â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–â–â–ƒâ–„â–„â–ƒâ–ƒâ–‚â–‚â–„â–ƒâ–ƒ
wandb:                          train/loss â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    train/perplexity â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                        train/time_s â–â–„â–„â–‚â–„â–â–„â–„â–ˆâ–„â–…â–ƒâ–„â–…â–…â–†â–ƒâ–…â–…â–†â–„â–ˆâ–„â–†â–„â–†â–…â–„â–…â–ƒâ–†â–…â–ƒâ–…â–„â–ƒâ–…â–‚â–…â–„
wandb:            val/inference_throughput â–‡â–†â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–‚â–„â–ƒâ–ƒâ–‚â–‚â–„â–…â–‚â–ƒâ–‚â–†â–ƒâ–‚â–ƒâ–ˆâ–‚â–„â–…â–„â–‚â–‚â–„â–ƒâ–…â–â–‡â–‚â–†
wandb:                            val/loss â–ˆâ–ˆâ–†â–†â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                      val/perplexity â–ˆâ–ˆâ–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                          val/time_s â–‚â–‚â–â–ƒâ–‚â–ƒâ–„â–„â–ƒâ–„â–ƒâ–„â–„â–…â–…â–ƒâ–…â–„â–„â–‚â–…â–…â–†â–‡â–„â–„â–ƒâ–‚â–„â–…â–…â–…â–†â–„â–„â–‡â–„â–ˆâ–†â–†
wandb: 
wandb: Run summary:
wandb:                          cpu/ram_gb 1.22071
wandb:                       gpu/active_gb 6.97155
wandb:                      gpu/active_pct 8.19259
wandb:                 gpu/bytes_per_token 106377.45312
wandb:                          gpu/gpu_gb 6.97155
wandb:                gpu/max_allocated_gb 26.79003
wandb:                     gpu/reserved_gb 27.39719
wandb:                    gpu/reserved_pct 32.19568
wandb:                        gpu/total_gb 85.09587
wandb:                                iter 9450
wandb:                                  lr 0.0001
wandb:                                 mfu 34.35007
wandb:   throughput/time_seconds_inference 0.82409
wandb: throughput/tokens_per_sec_inference 364.03717
wandb:     throughput/tokens_per_sec_train 860410.08502
wandb:   throughput/total_tokens_inference 300
wandb:          train/inference_throughput 1550385.41056
wandb:                          train/loss 4.18146
wandb:                    train/perplexity 65.46126
wandb:                        train/time_s 0.42271
wandb:            val/inference_throughput 1778249.57364
wandb:                            val/loss 4.16601
wandb:                      val/perplexity 64.45744
wandb:                          val/time_s 0.36854
wandb: 
wandb: ğŸš€ View run rope_0413_16:46:13 at: https://wandb.ai/gptnext/gptnext/runs/nhal89c1
wandb: â­ï¸ View project at: https://wandb.ai/gptnext/gptnext
wandb: Synced 8 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)
wandb: Find logs at: ./wandb/run-20250413_164618-nhal89c1/logs
