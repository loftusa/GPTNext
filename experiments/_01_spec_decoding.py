#%%
import torch
import torch.nn as nn

from _01_spec_decoding_model import GPT, GPTConfig


gpt_large = GPT.from_pretrained(model_type='gpt2-xl').to('cuda')
gpt2 = GPT.from_pretrained(model_type='gpt2').to('cuda')


#%%

gpt2
# %%
gpt_large
# %%
import tiktoken

gpt2.eval()
gpt_large.eval()
device = "cuda" if torch.cuda.is_available() else "cpu"
max_new_tokens = 10
temperature = 1.0

prompt = "1 2 3 4 5 6 7"

enc = tiktoken.get_encoding("gpt2")
idx = enc.encode(prompt)

idx = torch.tensor(idx, dtype=torch.long)[None, :].to(device)


output = gpt2.generate(idx=idx, max_new_tokens=max_new_tokens)
print(output)
enc.decode(output[0].tolist())
# %%
output_large = gpt_large.generate(idx=idx, max_new_tokens=max_new_tokens)
print(output_large)
enc.decode(output_large[0].tolist())
# %%
# code loosely appropriated from https://github.com/romsto/Speculative-Decoding/blob/main/sampling/speculative_decoding.py
# TODO: use KV cache

from torch.nn import functional as F

drafter, target = gpt2, gpt_large

# setup
gamma: int = 10  # number of drafts generated by the drafter at each step
eos_tokens_id = 50256
stop_tokens = torch.tensor([eos_tokens_id], dtype=torch.long, device=device).unsqueeze(1)
drafts_accepted, drafts_speculated = .0, .0
vocab_size = gpt2.config.vocab_size
pad_token_id = enc.encode(" ")[0]  # no pad token in gpt2 - default to space, use masks when relevant
prompt_len = idx.shape[1]
max_seq_len = 1024  # context window for all GPT2 models
total_len = min(max_seq_len, prompt_len + max_new_tokens)

input_ids = torch.full((1, total_len), pad_token_id, dtype=torch.long, device=device)
input_ids[0, :prompt_len] = idx

# generation
current_position = prompt_len
while current_position < total_len:
    corrected_gamma: int = min(gamma, total_len - current_position - 1)
    q = torch.zeros((1, corrected_gamma, vocab_size), device=device)  # stores draft tokens by small model

    # generate gamma drafts
    for k in range(corrected_gamma):
        draft_logits, _ = drafter(input_ids[..., :current_position + k])  # Mq is a GPT class, returns logits and loss from forward pass
        draft_logits = draft_logits[..., -1, :]  # last token logits
        draft_probs = F.softmax(draft_logits / temperature, dim=-1)
        q[0, k] = draft_probs.to(device)
        xi = torch.argmax(draft_probs, dim=-1).unsqueeze(1)  # greedy decoding
        input_ids[0, current_position+k] = xi
    drafts_speculated += corrected_gamma

    draft_logits_all, _ = target(input_ids[..., :current_position + corrected_gamma], return_all_logits=True)
    draft_logits = draft_logits_all[..., current_position - 1:current_position + corrected_gamma - 1, :]
    p = F.softmax(draft_logits / temperature, dim=-1)  # [1, gamma, vocab_size]

    # rejection sampling: last accepted draft position
    r = torch.rand(corrected_gamma, device=device)
    fractions = p/q
    n = corrected_gamma
    for i in range(corrected_gamma):
        if r[i] > fractions[0, i, input_ids[0, current_position + i]]:
            n = i
            break
    
    drafts_accepted += n

    # check if end token is in the drafts
    stop_locations = torch.nonzero(torch.eq(input_ids[..., current_position:current_position+n], stop_tokens)
    )

    if stop_locations.shape[0] > 0:
        stop_location = stop_locations[0, 1].item()
        output_ids, ratio = input_ids[0, prompt_len:current_position+stop_location+1].tolist(), drafts_accepted / drafts_speculated
        break

    if n == corrected_gamma:
        p_p = draft_logits_all[..., current_position+corrected_gamma-1, :]
        p_p = F.softmax(p_p / temperature, dim=-1)

    else:
        p_p = p[..., n, :]

    x = torch.argmax(p_p, dim=-1).unsqueeze(-1)

    input_ids[0, current_position+n:current_position+corrected_gamma] = (pad_token_id)
    input_ids[0, current_position+n] = x

    current_position += n + 1

    if torch.isin(x, stop_tokens):
        output_ids, ratio = input_ids[0, prompt_len:current_position].tolist(), drafts_accepted / drafts_speculated
    else:
        output_ids, ratio = input_ids[0, prompt_len:].tolist(), drafts_accepted / drafts_speculated

    print(f"Output: {enc.decode(output_ids)}")
    print(f"Ratio: {ratio}")
    break




#%%